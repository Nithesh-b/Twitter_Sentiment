{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project Domain #11 ( Sentiment Analysis in Twitter )\n",
    "\n",
    "Tweet text along with other features has been extracted from different from different sources (domain) using APIs.\n",
    "Each row of the dataset contains sentiment code (negative, positive and neutral embedded in Twit-id column. The task is to predict whether a tweet contains positive, negative, or neutral sentiment. This is a supervised learning task where given a text string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Basic Classification Test using Random Forest Classifier\n",
    "\n",
    "#### In this file we will be testing the data with basic random forest classification\n",
    "### Flow :-\n",
    "\n",
    "1. Read the Output file from Step 2 as the input for Step 3\n",
    "2. Check for missing values\n",
    "3. Drop NULL tweet rows\n",
    "4. Arranging columns required for text processing\n",
    "5. Setting the class column as the category for classification\n",
    "6. Setting the features and labels array from the data frame\n",
    "7. Vectoriztion using TF-IDF - Converting the Text to numbers to apply Machine Learning\n",
    "8. Model Performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input File - Step2_PreProcessing_Group33_Cleaned_Tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.18.5\n",
      "pandas: 1.0.5\n",
      "re: 2.2.1\n",
      "nltk: 3.5\n"
     ]
    }
   ],
   "source": [
    "# Library Imports\n",
    "\n",
    "import numpy as np \n",
    "print('numpy: {}'.format(np.__version__))\n",
    "\n",
    "import pandas as pd\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "\n",
    "import re\n",
    "print('re: {}'.format(re.__version__))\n",
    "\n",
    "import nltk\n",
    "print('nltk: {}'.format(nltk.__version__))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Input / Output - Folders where the input data will be read and output will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputdataFolder = \"/Users/aravindv/Wind/BITS Pilani/PGP - AIML/Course/Course 7 - Capstone Project\"\n",
    "OutputFolder = \"/Users/aravindv/Wind/BITS Pilani/PGP - AIML/Course/Course 7 - Capstone Project/Output\"\n",
    "MLOutfolder =  \"/Users/aravindv/Wind/BITS Pilani/PGP - AIML/Course/Course 7 - Capstone Project/ML\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reading the Pre-Processed data from round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30155, 11)\n"
     ]
    }
   ],
   "source": [
    "# Reading the Second Round PreProcessed Data\n",
    "# Data read - All Required data  are in datafolder\n",
    "cleaned_tweets_df = pd.read_csv(OutputFolder+\"/Step2_PreProcessing_Group33_Cleaned_Tweets.csv\")\n",
    "print(cleaned_tweets_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Finding missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the missing values in each column\n",
    "\n",
    "def find_missing_values_func(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"Selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataframe has 3 columns.\n",
      "There are 0 columns that have missing values.\n",
      "Empty DataFrame\n",
      "Columns: [Missing Values, % of Total Values]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Invoking the find_missing_values_func() with data frame of original tweets\n",
    "\n",
    "columnsWiseMissingValue = find_missing_values_func(cleaned_tweets_df) \n",
    "print(columnsWiseMissingValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dropping NULL tweet rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NULL Tweet-Text  rows as we use tweet text for text processing \n",
    "cleaned_tweets_df = cleaned_tweets_df.dropna(subset=[\"Clean_tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataframe has 3 columns.\n",
      "There are 0 columns that have missing values.\n",
      "Empty DataFrame\n",
      "Columns: [Missing Values, % of Total Values]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Check missing_values again , if any\n",
    "columnsWiseMissingValue = find_missing_values_func(cleaned_tweets_df) \n",
    "print(columnsWiseMissingValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id         object\n",
       "Clean_tweet      object\n",
       "class          category\n",
       "dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data Ready for Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Columns required for Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text processing We arrange the tweet_id, Clean_tweet and class\n",
    "ArrangeCollist = ['tweet_id', \n",
    "                  'Clean_tweet', \n",
    "                  'class' ]  # Label ]\n",
    "\n",
    "\n",
    "cleaned_tweets_df = cleaned_tweets_df.reindex(columns=ArrangeCollist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataframe has 3 columns.\n",
      "There are 0 columns that have missing values.\n",
      "Empty DataFrame\n",
      "Columns: [Missing Values, % of Total Values]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check missing_values again , if any\n",
    "columnsWiseMissingValue = find_missing_values_func(cleaned_tweets_df) \n",
    "print(columnsWiseMissingValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting the class column as the category for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets_df[\"class\"] = cleaned_tweets_df[\"class\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id         object\n",
       "Clean_tweet      object\n",
       "class          category\n",
       "dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>Clean_tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neu-GG-Tweet-11945</td>\n",
       "      <td>just land my ear hurt</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neu-GG-Tweet-11944</td>\n",
       "      <td>ouch follow asot tweetdeck exceed tweet limit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neu-GG-Tweet-11943</td>\n",
       "      <td>realli want to see one would go lmfao</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neu-GG-Tweet-11942</td>\n",
       "      <td>ahh repli random follow do not how sad haha</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neu-GG-Tweet-11941</td>\n",
       "      <td>awwww did not get hero</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                    Clean_tweet class\n",
       "0  neu-GG-Tweet-11945                          just land my ear hurt     2\n",
       "1  neu-GG-Tweet-11944  ouch follow asot tweetdeck exceed tweet limit     2\n",
       "2  neu-GG-Tweet-11943          realli want to see one would go lmfao     2\n",
       "3  neu-GG-Tweet-11942    ahh repli random follow do not how sad haha     2\n",
       "4  neu-GG-Tweet-11941                         awwww did not get hero     2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setting the features and labels array from the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cleaned_tweets_df.iloc[:, 1].values\n",
    "labels = cleaned_tweets_df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30053,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the features array again to remove special characters, single characters and numbers\n",
    "processed_features = []\n",
    "\n",
    "for sentence in range(0, len(features)):\n",
    "    # Remove all the special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
    "\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "\n",
    "    processed_features.append(processed_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vectoriztion using TF-IDF - Converting the Text to numbers to apply Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports for Vectoriztion\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Vectorizer with parameters\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "processed_features = vectorizer.fit_transform(processed_features).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe 80:20 preserve the distribution of class - use stratify\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0 ,stratify = cleaned_tweets_df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the Random Forest Classifier and fitting the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Predictions from the classifier\n",
    "predictions = text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1412  481   95]\n",
      " [ 508 1736  201]\n",
      " [ 407  817  354]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.71      0.65      1988\n",
      "           1       0.57      0.71      0.63      2445\n",
      "           2       0.54      0.22      0.32      1578\n",
      "\n",
      "    accuracy                           0.58      6011\n",
      "   macro avg       0.57      0.55      0.54      6011\n",
      "weighted avg       0.58      0.58      0.56      6011\n",
      "\n",
      "0.5825985692896357\n"
     ]
    }
   ],
   "source": [
    "# Printing the Metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----DONE----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
