{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Step1_PreProcessing_Group33_Twitter_Sentiment_Analysis.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PBY4eAfZPvc"
      },
      "source": [
        "# Capstone Project Domain #11 ( Sentiment Analysis in Twitter )\n",
        "\n",
        "Tweet text along with other features has been extracted from different from different sources (domain) using APIs.\n",
        "Each row of the dataset contains sentiment code (negative, positive and neutral embedded in Twit-id column. The task is to predict whether a tweet contains positive, negative, or neutral sentiment. This is a supervised learning task where given a text string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5cd6YijZPve"
      },
      "source": [
        "## Step 1 - Pre Processing\n",
        "\n",
        "#### In this file all the Pre-Processing Steps will be performed.\n",
        "### Flow :- \n",
        "\n",
        "1. Read the data from the Input File\n",
        "2. Finding the missing values in Each Column\n",
        "3. Creating the Label Columns from Tweet ID\n",
        "4. Dropping rows with NULL tweets\n",
        "5. Drop duplicate rows and tweet ID\n",
        "6. Filling NULL tweet source and tweet by columns\n",
        "7. Dropping Date Column\n",
        "8. Cleaning Tweet_Source column to follow same format\n",
        "9. Visualizing the data as PIE Charts\n",
        "10. Visualizing the data as Word Cloud to see patterns.\n",
        "11. Saving the Step 1 - Pre-Processing data to the file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgDgrYcnZPve"
      },
      "source": [
        "### Input File - Base_tweets_DataSetV3.xlsx\n",
        "### Output File - Step1_PreProcessing_Group33_Cleaned_Tweets.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjrT70kJZPvf",
        "outputId": "8a728a22-d968-4512-88c1-cf765290afbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Library Imports\n",
        "\n",
        "import numpy as np \n",
        "print('numpy: {}'.format(np.__version__))\n",
        "\n",
        "import pandas as pd\n",
        "print('pandas: {}'.format(pd.__version__))\n",
        "\n",
        "import re\n",
        "print('re: {}'.format(re.__version__))\n",
        "\n",
        "import nltk\n",
        "print('nltk: {}'.format(nltk.__version__))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numpy: 1.18.5\n",
            "pandas: 1.1.2\n",
            "re: 2.2.1\n",
            "nltk: 3.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwZ9GPXAZPvj",
        "outputId": "805c4b81-d8b0-442d-d1a1-6be2e4ee9291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Getting the Stop Words and Other Text Processing Libraries\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from termcolor import colored\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ9NzyPAZPvm"
      },
      "source": [
        "### Data Input / Output - Folders where the input data will be read and output will be stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL8I6o2GZPvn"
      },
      "source": [
        "InputdataFolder = \"/Users/aravindv/Wind/BITS Pilani/PGP - AIML/Course/Course 7 - Capstone Project\"\n",
        "OutputFolder = \"/Users/aravindv/Wind/BITS Pilani/PGP - AIML/Course/Course 7 - Capstone Project/Output\"\n",
        "MLOutfolder =  \"/Users/aravindv/Wind/BITS Pilani/PGP - AIML/Course/Course 7 - Capstone Project/ML\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCry7YFBZPvp",
        "outputId": "b261796a-a21c-4ddc-dfbf-8b9c6b58c011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "# Reading the data file and storing in the dataframe tweets_original_df\n",
        "tweets_original_df = pd.read_excel(InputdataFolder+\"/Base_tweets_DataSetV3.xlsx\")\n",
        "print(tweets_original_df.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-be16a03f2d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reading the data file and storing in the dataframe tweets_original_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweets_original_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputdataFolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/Base_tweets_DataSetV3.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_original_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/aravindv/Wind/BITS Pilani/PGP - AIML/Course/Course 7 - Capstone Project/Base_tweets_DataSetV3.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nHsnNziZPvs"
      },
      "source": [
        "# Peek at the data\n",
        "print(tweets_original_df.head(5))\n",
        "print(\"--------------------------------\")\n",
        "print(tweets_original_df.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehx9Pj99ZPvv"
      },
      "source": [
        "## Pre - Processing Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RebxW211ZPvw"
      },
      "source": [
        "### 1 . Finding the missing values in each column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RQ6ejPbZPvw"
      },
      "source": [
        "# Function to find the missing values in each column\n",
        "\n",
        "def find_missing_values_func(df):\n",
        "        mis_val = df.isnull().sum()\n",
        "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
        "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "        mis_val_table_ren_columns = mis_val_table.rename(\n",
        "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
        "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
        "        '% of Total Values', ascending=False).round(1)\n",
        "        print (\"Selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
        "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
        "              \" columns that have missing values.\")\n",
        "        return mis_val_table_ren_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dr8F1v-ZPv0"
      },
      "source": [
        "# Invoking the find_missing_values_func() with data frame of original tweets\n",
        "\n",
        "columnsWiseMissingValue = find_missing_values_func(tweets_original_df) \n",
        "print(columnsWiseMissingValue)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s7yNrWYZPv2"
      },
      "source": [
        "### 2. Creating the Label Column from the Tweet ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoORK0b7ZPv3"
      },
      "source": [
        "# The dataset contains a column called tweet_id which is used to get the Label \n",
        "# Extract label_id from tweet_id - First 3 character label_id\n",
        "\n",
        "label = list(tweets_original_df['tweet_id'].str[:3])\n",
        "tweets_original_df['label_id'] = pd.Series(label).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxbUOVj2ZPv5"
      },
      "source": [
        "# List of Tweets with Label Column Added\n",
        "display(tweets_original_df.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "457g_OExZPv9"
      },
      "source": [
        "### 3. Drop rows which contain NULL tweets as Text Processing will be done on the tweet column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgqnS9GJZPv9"
      },
      "source": [
        "# Drop NULL Tweet-Text  rows as we use tweet text for text processing \n",
        "tweets_original_df = tweets_original_df.dropna(subset=[\"Tweet\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OonhvxIZPwA"
      },
      "source": [
        "### 4. Drop Duplicate rows and tweet_id  - To be done"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GadWeS1ZTzL"
      },
      "source": [
        "tweets_df.drop_duplicates(subset =\"tweet_id\", keep = False, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm6WFdCLZPwA"
      },
      "source": [
        "### 5. Filling Null Tweet Source and Tweet By columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqywaznCZPwB"
      },
      "source": [
        "# Inpute Null Tweet_source as OTHER\n",
        "column = 'Tweet_source'\n",
        "tweets_original_df[column] = tweets_original_df[column].fillna(\"OTHER\")\n",
        "\n",
        "# Inpute Null Tweeted-By as Unknown\n",
        "column = 'Tweeted-By'\n",
        "tweets_original_df[column] = tweets_original_df[column].fillna(\"Unknown\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBp-vh05ZPwD"
      },
      "source": [
        "#Check missing_values again , if any\n",
        "columnsWiseMissingValue = find_missing_values_func(tweets_original_df) \n",
        "print(columnsWiseMissingValue)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j_xI7BYZPwG"
      },
      "source": [
        "### 6. Dropping the Date Column as it is not required for modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTaUh1G1ZPwH"
      },
      "source": [
        "#Dropping Date Created column from main dataframe as it has no use\n",
        "tweets_original_df = tweets_original_df.drop([\"Date Created\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKzGq9IcZPwJ"
      },
      "source": [
        "print(tweets_original_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGT_D6YJZPwM"
      },
      "source": [
        "tweets_original_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CADn7eMZPwP"
      },
      "source": [
        "# Configuring the Plot Sizes\n",
        "plot_size = plt.rcParams[\"figure.figsize\"] \n",
        "print(plot_size[0]) \n",
        "print(plot_size[1])\n",
        "\n",
        "plot_size[0] = 8\n",
        "plot_size[1] = 6\n",
        "plt.rcParams[\"figure.figsize\"] = plot_size "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04fK1yaCZPwR"
      },
      "source": [
        " tweets_original_df[\"Tweet_source\"].value_counts() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqkncgmiZPwU"
      },
      "source": [
        "### 7. Cleaning  Tweet_source column to follow a single format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYl9ta4zZPwU"
      },
      "source": [
        "# Replace(['S-5','s5'] with 'S5')\n",
        "tweets_original_df['Tweet_source'] = tweets_original_df['Tweet_source'].replace(['S-5'],'S5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4hGVgElZPwX"
      },
      "source": [
        "tweets_original_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlms3AvyZPwa"
      },
      "source": [
        "# Visualizing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfdBt-v2ZPwb"
      },
      "source": [
        "# Plot Distribution\n",
        "tweets_original_df.Tweet_source.value_counts().plot(kind='pie', autopct='%1.0f%%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7NpZHsmZPwd"
      },
      "source": [
        "## Label distribution - Positive, Negative, & Neutral"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wATVDq1vZPwd"
      },
      "source": [
        "# Check label_id \n",
        "tweets_original_df[\"label_id\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZsR3WYcZPwg"
      },
      "source": [
        "# Refine the graph\n",
        "\n",
        "class_count = tweets_original_df['label_id'].value_counts() # Returned in descending order [4, 0]\n",
        "\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.bar(['Negative' , 'Positive' , 'Neutral'], height = class_count.values, color = ['b', 'g', 'r'])\n",
        "for i, v in enumerate(class_count.values):\n",
        "    plt.text(i - 0.1, v+300 , str(v))\n",
        "    \n",
        "plt.xlabel('Tweet sentiment')\n",
        "plt.ylabel('Number of tweets')\n",
        "plt.title('Count of tweets for each sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vzAz8lAZPwy"
      },
      "source": [
        "#### Country wise positive , Negative and Neutral - Map If possible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AowlZk04a0Tu"
      },
      "source": [
        "tweets_pos = tweets_df[(tweets_df.label_id.isin([\"pos\"]))]\n",
        "tweets_neu = tweets_df[(tweets_df.label_id.isin([\"neu\"]))]\n",
        "tweets_neg = tweets_df[(tweets_df.label_id.isin([\"neg\"]))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qTgPdXwa3Jt"
      },
      "source": [
        "tweets_pos[\"Country\"].value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"red\", \"yellow\", \"blue\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKkFU4-Ca6-_"
      },
      "source": [
        "tweets_neg[\"Country\"].value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"red\", \"yellow\", \"blue\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_-mhtEaa95D"
      },
      "source": [
        "tweets_neu[\"Country\"].value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"red\", \"yellow\", \"blue\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbxQZOmiZPwi"
      },
      "source": [
        "### Convert label_id to numeric  as - {'neu' : 2, 'pos' : 0, 'neg' : 1} "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywNo_mKGZPwi"
      },
      "source": [
        "tweets_original_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgHiX7yzZPwl"
      },
      "source": [
        "# Create the dictionary \n",
        "class_dictionary = {'neu' : 2, 'pos' : 0, 'neg' : 1} \n",
        "  \n",
        "# Add a new column named 'Price' \n",
        "tweets_original_df['class'] = tweets_original_df['label_id'].map(class_dictionary) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MnmuuatZPwo"
      },
      "source": [
        "#Dropping label-id  column from main dataframe as it has been converted to class column as numeric\n",
        "tweets_original_df = tweets_original_df.drop([\"label_id\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXC1LFW8ZPwq"
      },
      "source": [
        "# Check label (class in numeric) distribution - # 2 = Neutral, 0 = Positive  , 1 = Negative\n",
        "tweets_original_df[\"class\"].value_counts()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BooRT7O0ZPws"
      },
      "source": [
        "tweets_original_df[\"class\"].value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"red\", \"yellow\", \"blue\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQR7fnNKZPwu"
      },
      "source": [
        "print(tweets_original_df.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce5xxTS7ZPwy"
      },
      "source": [
        "#### Explore Negative tweet and its catergory ( column - Tweet-Class_category-Code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW1xrMAcaO80"
      },
      "source": [
        "tweets_neg[\"Tweet-Class_category-Code\"].value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"red\", \"yellow\", \"blue\",\"green\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_mn0_jWZPwy"
      },
      "source": [
        "##### Explore distribution \"no of twwets\" (column - retweet_count) for each label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY5WJ21yaWrY"
      },
      "source": [
        "tweets_df[\"retweet_count\"].value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"red\", \"yellow\", \"blue\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p7Ub6P6ZPwz"
      },
      "source": [
        "# Tweet pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf3fl1jSZPwz"
      },
      "source": [
        "### WordCloud of each class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMpvKuJMZPw0"
      },
      "source": [
        "positive_tweets = ' '.join(tweets_original_df[tweets_original_df['class'] == 0]['Tweet'].str.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0VX-yaXZPw2"
      },
      "source": [
        "neutral_tweets = ' ' .join(tweets_original_df[tweets_original_df['class'] == 2]['Tweet'].str.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2HU8_spZPw4"
      },
      "source": [
        "negative_tweets = ' '.join(tweets_original_df[tweets_original_df['class'] == 1]['Tweet'].str.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L69vhjniZPw6"
      },
      "source": [
        "# pip install wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iYqHZPXZPw9"
      },
      "source": [
        "## POSITIVE Tweets Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeAo9idNZPw-"
      },
      "source": [
        "# \"stop words\", in simple terms it refers to the most common words in a language. \n",
        "# These are typically uninformative words, such as \"the\" or \"and\", for example, \n",
        "# that are thus removed during preprocessing in many Natural Language Processing (NLP) applications.\n",
        "wordcloud = WordCloud(stopwords = STOPWORDS, background_color = \"white\", max_words = 1000).generate(positive_tweets)\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Positive tweets Wordcloud\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHx8b_PqZPxA"
      },
      "source": [
        "## NEGATIVE Tweets Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYKsm_nyZPxA"
      },
      "source": [
        "wordcloud = WordCloud(stopwords = STOPWORDS, background_color = \"white\", max_words = 1000).generate(negative_tweets)\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Negative tweets Wordcloud\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW5hz801ZPxC"
      },
      "source": [
        "## NEUTRAL Tweets Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scaE2-T3ZPxD"
      },
      "source": [
        "wordcloud = WordCloud(stopwords = STOPWORDS, background_color = \"white\", max_words = 1000).generate(neutral_tweets)\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Neutral tweets Wordcloud\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoQaT5FUZPxF"
      },
      "source": [
        "#Check null before splitting\n",
        "columnsWiseMissingValue = find_missing_values_func(tweets_original_df) \n",
        "print(columnsWiseMissingValue)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5LnrmT_ZPxH"
      },
      "source": [
        "print(colored(\"Class distribution:\", \"yellow\"))\n",
        "print(tweets_original_df['class'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdJR_rytZPxK"
      },
      "source": [
        "# Saving Step 1 Pre-Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMPfVWHhZPxK"
      },
      "source": [
        "#Save first round cleaned tweets_original_df\n",
        "tweets_original_df.to_csv(OutputFolder+\"/Step1_PreProcessing_Group33_Cleaned_Tweets.csv\", index = False)\n",
        "print(colored(\"DATA SAVED\", \"green\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPQwFabzZPxM"
      },
      "source": [
        "# ----DONE----"
      ]
    }
  ]
}