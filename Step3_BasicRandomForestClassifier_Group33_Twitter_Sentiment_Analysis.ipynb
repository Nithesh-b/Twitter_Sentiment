{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Copy of Step3_BasicRandomForestClassifier_Group33_Twitter_Sentiment_Analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nithesh-b/Twitter_Sentiment/blob/post-viva/Step3_BasicRandomForestClassifier_Group33_Twitter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V8WKG3dZXmG"
      },
      "source": [
        "# Capstone Project Domain #11 ( Sentiment Analysis in Twitter )\n",
        "\n",
        "Tweet text along with other features has been extracted from different from different sources (domain) using APIs.\n",
        "Each row of the dataset contains sentiment code (negative, positive and neutral embedded in Twit-id column. The task is to predict whether a tweet contains positive, negative, or neutral sentiment. This is a supervised learning task where given a text string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy65x0omZXmH"
      },
      "source": [
        "## Step 3 - Basic Classification Test using Random Forest Classifier\n",
        "\n",
        "#### In this file we will be testing the data with basic random forest classification\n",
        "### Flow :-\n",
        "\n",
        "1. Read the Output file from Step 2 as the input for Step 3\n",
        "2. Check for missing values\n",
        "3. Drop NULL tweet rows\n",
        "4. Arranging columns required for text processing\n",
        "5. Setting the class column as the category for classification\n",
        "6. Setting the features and labels array from the data frame\n",
        "7. Vectoriztion using TF-IDF - Converting the Text to numbers to apply Machine Learning\n",
        "8. Model Performance metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFgDqTleZXmI"
      },
      "source": [
        "### Input File - Step2_PreProcessing_Group33_Cleaned_Tweets.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR-gmbVFZXmJ",
        "outputId": "bd6f2904-023c-488a-8d15-c1189add3f5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Library Imports\n",
        "\n",
        "import numpy as np \n",
        "print('numpy: {}'.format(np.__version__))\n",
        "\n",
        "import pandas as pd\n",
        "print('pandas: {}'.format(pd.__version__))\n",
        "\n",
        "import re\n",
        "print('re: {}'.format(re.__version__))\n",
        "\n",
        "import nltk\n",
        "print('nltk: {}'.format(nltk.__version__))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numpy: 1.18.5\n",
            "pandas: 1.1.3\n",
            "re: 2.2.1\n",
            "nltk: 3.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-qLOv5eZXmN"
      },
      "source": [
        "### Data Input / Output - Folders where the input data will be read and output will be stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeSuWaIMZXmO"
      },
      "source": [
        "InputdataFolder = \"/content/sample_data/\"\n",
        "OutputFolder = \"/content/sample_data/outputs\"\n",
        "MLOutfolder =  \"/content/sample_data/outputs\""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sGU_xE4ZXmS"
      },
      "source": [
        "##  Reading the Pre-Processed data from round 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNNVInaxZXmT",
        "outputId": "09b56361-c292-4e28-e67b-ff26d6f9c79c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Reading the Second Round PreProcessed Data\n",
        "# Data read - All Required data  are in datafolder\n",
        "cleaned_tweets_df = pd.read_csv(OutputFolder+\"/Step2_PreProcessing_Group33_Cleaned_Tweets.csv\")\n",
        "print(cleaned_tweets_df.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30155, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIiiq7XvZXmW"
      },
      "source": [
        "### 1. Finding missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy0B6RchZXmW"
      },
      "source": [
        "# Function to find the missing values in each column\n",
        "\n",
        "def find_missing_values_func(df):\n",
        "        mis_val = df.isnull().sum()\n",
        "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
        "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "        mis_val_table_ren_columns = mis_val_table.rename(\n",
        "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
        "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
        "        '% of Total Values', ascending=False).round(1)\n",
        "        print (\"Selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
        "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
        "              \" columns that have missing values.\")\n",
        "        return mis_val_table_ren_columns"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24oEAa_IZXmZ",
        "outputId": "106c0e70-33a5-494f-c4a3-8e3681001547",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Invoking the find_missing_values_func() with data frame of original tweets\n",
        "\n",
        "columnsWiseMissingValue = find_missing_values_func(cleaned_tweets_df) \n",
        "print(columnsWiseMissingValue)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected dataframe has 11 columns.\n",
            "There are 2 columns that have missing values.\n",
            "             Missing Values  % of Total Values\n",
            "Clean_tweet             102                0.3\n",
            "Tweet                    57                0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKF4iFC0ZXmc"
      },
      "source": [
        "### 2. Dropping NULL tweet rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ktZdiKkZXmc"
      },
      "source": [
        "# Drop NULL Tweet-Text  rows as we use tweet text for text processing \n",
        "cleaned_tweets_df = cleaned_tweets_df.dropna(subset=[\"Clean_tweet\"])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAaS1iqCZXmg",
        "outputId": "06242198-e07b-48f8-e8fe-755bf2ea0736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Check missing_values again , if any\n",
        "columnsWiseMissingValue = find_missing_values_func(cleaned_tweets_df) \n",
        "print(columnsWiseMissingValue)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected dataframe has 11 columns.\n",
            "There are 0 columns that have missing values.\n",
            "Empty DataFrame\n",
            "Columns: [Missing Values, % of Total Values]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g47VmLSZXmj",
        "outputId": "826e2e18-6632-442b-dfca-e9821296dbe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cleaned_tweets_df.dtypes"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tweet_id                     object\n",
              "SourceDataBase               object\n",
              "OS                           object\n",
              "Tweet-Class_category-Code     int64\n",
              "Tweet_source                 object\n",
              "Tweeted-By                   object\n",
              "retweet_count                 int64\n",
              "Tweet                        object\n",
              "Clean_tweet                  object\n",
              "Country                      object\n",
              "class                         int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpBeQawdZXmm"
      },
      "source": [
        "## Getting Data Ready for Text Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X43bwT3ZXmm"
      },
      "source": [
        "### 1. Columns required for Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdSD96nNZXmm"
      },
      "source": [
        "# For text processing We arrange the tweet_id, Clean_tweet and class\n",
        "ArrangeCollist = ['tweet_id', \n",
        "                  'Clean_tweet', \n",
        "                  'class' ]  # Label ]\n",
        "\n",
        "\n",
        "cleaned_tweets_df = cleaned_tweets_df.reindex(columns=ArrangeCollist)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK396g34ZXmp",
        "outputId": "ae74e7d0-6501-481c-9d17-c0592d4a2ff3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check missing_values again , if any\n",
        "columnsWiseMissingValue = find_missing_values_func(cleaned_tweets_df) \n",
        "print(columnsWiseMissingValue)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected dataframe has 3 columns.\n",
            "There are 0 columns that have missing values.\n",
            "Empty DataFrame\n",
            "Columns: [Missing Values, % of Total Values]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdKhHsjAZXms"
      },
      "source": [
        "### 2. Setting the class column as the category for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj7ovrmGZXms"
      },
      "source": [
        "cleaned_tweets_df[\"class\"] = cleaned_tweets_df[\"class\"].astype('category')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79f9xgDdZXmu",
        "outputId": "62b9736b-32b3-4ad2-b6fe-800d11471ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cleaned_tweets_df.dtypes"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tweet_id         object\n",
              "Clean_tweet      object\n",
              "class          category\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BPStOl7ZXmx",
        "outputId": "6aea3118-925d-4a91-c007-ee165afc8602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "cleaned_tweets_df.head(5)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>Clean_tweet</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neu-GG-Tweet-11945</td>\n",
              "      <td>just land my ear hurt</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neu-GG-Tweet-11944</td>\n",
              "      <td>ouch follow asot tweetdeck exceed tweet limit</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neu-GG-Tweet-11943</td>\n",
              "      <td>realli want to see one would go lmfao</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>neu-GG-Tweet-11942</td>\n",
              "      <td>ahh repli random follow do not how sad haha</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neu-GG-Tweet-11941</td>\n",
              "      <td>awwww did not get hero</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id                                    Clean_tweet class\n",
              "0  neu-GG-Tweet-11945                          just land my ear hurt     2\n",
              "1  neu-GG-Tweet-11944  ouch follow asot tweetdeck exceed tweet limit     2\n",
              "2  neu-GG-Tweet-11943          realli want to see one would go lmfao     2\n",
              "3  neu-GG-Tweet-11942    ahh repli random follow do not how sad haha     2\n",
              "4  neu-GG-Tweet-11941                         awwww did not get hero     2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xctnuvhZXm0"
      },
      "source": [
        "### 3. Setting the features and labels array from the data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hgARvPsZXm0"
      },
      "source": [
        "features = cleaned_tweets_df.iloc[:, 1].values\n",
        "labels = cleaned_tweets_df.iloc[:, -1].values"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZyPR93VZXm3",
        "outputId": "620cb63b-f275-4481-a7f8-79b8d61424f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "features.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30053,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1RDwX3ZXm6"
      },
      "source": [
        "# Processing the features array again to remove special characters, single characters and numbers\n",
        "processed_features = []\n",
        "\n",
        "for sentence in range(0, len(features)):\n",
        "    # Remove all the special characters\n",
        "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
        "\n",
        "    # remove all single characters\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
        "\n",
        "    # Converting to Lowercase\n",
        "    processed_feature = processed_feature.lower()\n",
        "\n",
        "    processed_features.append(processed_feature)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ReoiNitZXm8"
      },
      "source": [
        "### 4. Vectoriztion using TF-IDF - Converting the Text to numbers to apply Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWZsIHJ0ZXm9",
        "outputId": "d193e61f-729c-4a12-c010-5b22c100a009",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Library Imports for Vectoriztion\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3HNZyW-ZXm_"
      },
      "source": [
        "# Initializing the Vectorizer with parameters\n",
        "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
        "processed_features = vectorizer.fit_transform(processed_features).toarray()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpkG72jeZXnC"
      },
      "source": [
        "#### Creating the Training and Test Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGuhQavTZXnD"
      },
      "source": [
        "# Split the dataframe 80:20 preserve the distribution of class - use stratify\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0 ,stratify = cleaned_tweets_df['class'])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2z1mRbkZXnF",
        "outputId": "fc19e88a-4d20-49e4-c696-394d0443283e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Initializing the Random Forest Classifier and fitting the model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "text_classifier.fit(X_train, y_train)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7C4MShYZXnH"
      },
      "source": [
        "# Calculating the Predictions from the classifier\n",
        "predictions = text_classifier.predict(X_test)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcjaQ3MIZXnK",
        "outputId": "0571bbdc-553f-4157-c217-ae1f4ad8ddc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Printing the Metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
        "\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "print(classification_report(y_test,predictions))\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1415  476   97]\n",
            " [ 505 1743  197]\n",
            " [ 415  810  353]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.71      0.65      1988\n",
            "           1       0.58      0.71      0.64      2445\n",
            "           2       0.55      0.22      0.32      1578\n",
            "\n",
            "    accuracy                           0.58      6011\n",
            "   macro avg       0.58      0.55      0.54      6011\n",
            "weighted avg       0.58      0.58      0.56      6011\n",
            "\n",
            "0.5840958243220762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ7Ucl2mZXnM"
      },
      "source": [
        "# ----DONE----"
      ]
    }
  ]
}